+++
title = "Agents Conversationnels Animés pour l’entrainement social : modèle computationnel de l’expression d’attitudes sociales par des séquences de signaux non-verbaux"
date = 2015-04-21
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["M., Chollet"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["3"]

# Publication name and optional abbreviated version.
publication = "PhD Thesis"
publication_short = ""

# Abstract and optional shortened version.
abstract = "The Embodied Conversational Agents (ECAs) used in social training must be able to simulate all the different social situations that a learner has to train to. Depending on the application, the ECAs must then be able to express various emotions or various attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. However, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with other signals : for instance, a smile followed by a gaze aversion and a head aversion does not signal amusement, but embarrassment. Non-verbal behavior planning models for ECAs should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The main contribution of this thesis is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of attitude variations from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. <br> Another consideration in the design of social training systems is to check that users do improve their social skills while using such systems. We investigated the use of ECAs to build a virtual audience aimed at improving users’ public speaking skills. Another contribution of this thesis is the proposal of an architecture for interactive virtual audiences that provide real-time feedback to the learner according to his public speaking performance, and to have evaluated three different feedback strategies."
abstract_short = ""

# Is this a featured publication? (true/false)
featured = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = ["Tardis"]

# Slides (optional).
#   Associate this page with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = ["Virtual Agents"]

# Links (optional).
url_pdf = "/publication/phd15/Chollet_phd2015.pdf"
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Digital Object Identifier (DOI)
doi = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = ""
+++
